{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nTAz7gv1O9Ys"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import _pickle as pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RmjWJnqhQJFr"
      },
      "outputs": [],
      "source": [
        "from all_S2S_models import main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1364/3992818651.py:1: ParserWarning: Both a converter and dtype were specified for column Date_Time - only the converter will be used.\n",
            "  dataset = pd.read_csv(\n",
            "/tmp/ipykernel_1364/3992818651.py:1: ParserWarning: Both a converter and dtype were specified for column Date - only the converter will be used.\n",
            "  dataset = pd.read_csv(\n"
          ]
        }
      ],
      "source": [
        "dataset = pd.read_csv(\n",
        "\t\t'../data/run-of-river_production_load.csv', \n",
        "\t\tdtype=np.float32,\n",
        "\t\tconverters={\n",
        "\t\t\t\"Date_Time\": pd.to_datetime,\n",
        "\t\t\t\"Date\": pd.to_datetime\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bCK2jaUpQTdL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transforming data to 0 mean and unit var\n",
            "Generating training and test data...\n",
            "Created 65049 train samples and 15975 test samples\n",
            "Training shapes: (65049, 192, 44) (65049, 192, 1)\n",
            "Training shapes: (15975, 192, 44) (15975, 192, 1)\n",
            "MODEL ARCHITECTURE IS: \n",
            "S2S_BA_Model(\n",
            "  (Ecell): LSTMCell(44, 48)\n",
            "  (Dcell): LSTMCell(49, 48)\n",
            "  (Wattn_energies): Linear(in_features=96, out_features=48, bias=True)\n",
            "  (WValue): Linear(in_features=48, out_features=1, bias=True)\n",
            "  (Wout): Linear(in_features=97, out_features=48, bias=True)\n",
            ")\n",
            "\n",
            "Model parameters are on cuda: False\n",
            "\n",
            "Starting training...\n",
            "Epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/509 [00:22<3:08:35, 22.28s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m main(\n\u001b[1;32m      2\u001b[0m     dataset\u001b[39m=\u001b[39mdataset,\n\u001b[1;32m      3\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m# for reproducability\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     cuda\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available(), \u001b[39m# change to True if available on your platform\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     cell_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlstm\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     attention_model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBA\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     la_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      8\u001b[0m     window_source_size\u001b[39m=\u001b[39m\u001b[39m96\u001b[39m \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[1;32m      9\u001b[0m     window_target_size\u001b[39m=\u001b[39m\u001b[39m96\u001b[39m \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[1;32m     10\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     11\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m,\n\u001b[1;32m     12\u001b[0m     hs\u001b[39m=\u001b[39m\u001b[39m48\u001b[39m, \u001b[39m# overall training parameters\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     save_model\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m   )\n\u001b[1;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mresults_sparse.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     17\u001b[0m   file\u001b[39m.\u001b[39mwrite(pickle\u001b[39m.\u001b[39mdumps(results))\n",
            "File \u001b[0;32m~/src/ds5220-team-2/s2s-attention-energy-forecasting/all_S2S_models.py:689\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(dataset, seed, cuda, cell_type, attention_model, la_method, window_source_size, window_target_size, epochs, batch_size, hs, save_model)\u001b[0m\n\u001b[1;32m    686\u001b[0m     preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(pred_Value, h, WINDOW_TARGET_SIZE)\n\u001b[1;32m    688\u001b[0m \u001b[39melif\u001b[39;00m attention_model \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mBA\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 689\u001b[0m     preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    690\u001b[0m         pred_Value, h, encoder_outputs, WINDOW_TARGET_SIZE)\n\u001b[1;32m    692\u001b[0m \u001b[39melif\u001b[39;00m attention_model \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLA\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    693\u001b[0m     preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m    694\u001b[0m         pred_Value, h, encoder_outputs, WINDOW_TARGET_SIZE)\n",
            "File \u001b[0;32m~/src/ds5220-team-2/s2s-attention-energy-forecasting/all_S2S_models.py:299\u001b[0m, in \u001b[0;36mS2S_BA_Model.predict\u001b[0;34m(self, pred_Value, h, encoder_outputs, target_length)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(target_length):\n\u001b[1;32m    298\u001b[0m     h_copies \u001b[39m=\u001b[39m h[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mexpand(encoder_outputs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 299\u001b[0m     energies \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mWattn_energies(\n\u001b[1;32m    300\u001b[0m         torch\u001b[39m.\u001b[39;49mcat((h_copies, encoder_outputs), \u001b[39m2\u001b[39;49m)))\n\u001b[1;32m    301\u001b[0m     score \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv \u001b[39m*\u001b[39m energies, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    302\u001b[0m     attn_weights \u001b[39m=\u001b[39m score\u001b[39m.\u001b[39mt()\n",
            "File \u001b[0;32m~/miniconda3/envs/ds5220/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/ds5220/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "results = main(\n",
        "    dataset=dataset,\n",
        "    seed=0, # for reproducability\n",
        "    cuda=torch.cuda.is_available(), # change to True if available on your platform\n",
        "    cell_type='lstm',\n",
        "    attention_model='BA',\n",
        "    la_method='none', \n",
        "    window_source_size=96 * 2,\n",
        "    window_target_size=96 * 2,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    hs=48, # overall training parameters\n",
        "    save_model=True\n",
        "  )\n",
        "\n",
        "with open('results_sparse.txt', 'wb') as file:\n",
        "  file.write(pickle.dumps(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ds5220",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "9dfb4e410c48678b2b8c9d8792f0b47c7585d762fe821f4d959e4a804ac8de56"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
